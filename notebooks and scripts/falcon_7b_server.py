# -*- coding: utf-8 -*-
"""falcon_7b_server.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wvHW3IPzo5C4c55PIx2OK4CMCD5iCH8j
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset,DatasetDict
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer
import transformers
# from peft import LoraConfig
from transformers import TrainingArguments
from trl import SFTTrainer

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

trained_model = "gopeshravi/quenerator_falcon"

model = AutoModelForCausalLM.from_pretrained(
    trained_model,
    quantization_config=bnb_config,
    trust_remote_code=True
)
model.config.use_cache = False

tokenizer = AutoTokenizer.from_pretrained(trained_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    # torch_dtype=torch.bfloat16,
    torch_dtype=torch.float16,
    trust_remote_code=True,
    device_map="auto",
)
def query_gen(input: str):
    sequences = pipeline(
        input,
        max_length=100,  #200,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
    )
    return sequences[0]['generated_text']

# for ix,seq in enumerate(sequences):
#     print(ix,seq[0]['generated_text'])

while(True):
    usr_query = input("Enter query:")
    if usr_query != 'quit':
        sql_query = query_gen(usr_query)
        print(sql_query)
    else:
        break